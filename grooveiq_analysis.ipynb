{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afe6190",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from data import *\n",
    "from utils.analysis import *\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6257b3",
   "metadata": {},
   "source": [
    "## Absolute Measurement Analysis Between 2Bar-GrooveIQ and 2bar-GMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6e4c4",
   "metadata": {},
   "source": [
    "### GrooveIQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"dataset/serialized\"\n",
    "giq_paths    = ['merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_val.pkl', 'merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_test.pkl', 'merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_train.pkl']\n",
    "name         = \"GrooveIQ\"\n",
    "\n",
    "dataset = []\n",
    "for path in giq_paths:\n",
    "    with open(os.path.join(dataset_root, path), \"rb\") as f:\n",
    "        dataset.extend(pickle.load(f))\n",
    "\n",
    "GrooveIQStats = DataStats()\n",
    "GrooveIQStats.set_name(name = name)\n",
    "\n",
    "for datapoint in tqdm(dataset):\n",
    "    GrooveIQStats.accumulate_dict(datapoint)\n",
    "\n",
    "#GrooveIQStats.visualize()\n",
    "#GrooveIQStats.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68145181",
   "metadata": {},
   "source": [
    "#### Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Absolute Measurements\n",
    "giq_eval_dict = defaultdict(list)\n",
    "\n",
    "for sample in tqdm(GrooveIQStats.all_samples):\n",
    "    descriptors = sample.descriptors.descriptors\n",
    "    for key, value in descriptors.items():\n",
    "        giq_eval_dict[key].append(value)\n",
    "\n",
    "giq_eval_dict = {key: np.array(value) for key, value in giq_eval_dict.items()}\n",
    "\n",
    "for key, value in giq_eval_dict.items():\n",
    "    print(f\"{key:30}: mean={np.mean(value):.2f}, std={np.std(value):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066f906",
   "metadata": {},
   "source": [
    "### GMD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GMD_paths    = ['2barGMD.pkl']\n",
    "name         = \"2barGMD\"\n",
    "\n",
    "dataset = []\n",
    "for path in GMD_paths:\n",
    "    with open(os.path.join(dataset_root, path), \"rb\") as f:\n",
    "        dataset.extend(pickle.load(f))\n",
    "\n",
    "GMDStats = DataStats()\n",
    "GMDStats.set_name(name = name)\n",
    "\n",
    "for datapoint in tqdm(dataset):\n",
    "    GMDStats.accumulate_dict(datapoint)\n",
    "\n",
    "#GMDStats.summarize()\n",
    "#GMDStats.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae3cd3",
   "metadata": {},
   "source": [
    "#### Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Absolute Measurements\n",
    "gmd_eval_dict = defaultdict(list)\n",
    "\n",
    "for sample in tqdm(GMDStats.all_samples):\n",
    "    descriptors = sample.descriptors.descriptors\n",
    "    for key, value in descriptors.items():\n",
    "        gmd_eval_dict[key].append(value)\n",
    "\n",
    "for key, value in gmd_eval_dict.items():\n",
    "    print(f\"{key:30}: mean={np.mean(value):.2f}, std={np.std(value):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf82c6",
   "metadata": {},
   "source": [
    "## Note Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"dataset/serialized\"\n",
    "giq_paths    = ['merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_val.pkl', 'merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_test.pkl', 'merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_train.pkl']\n",
    "name         = \"GrooveIQ\"\n",
    "\n",
    "dataset = []\n",
    "for path in giq_paths:\n",
    "    with open(os.path.join(dataset_root, path), \"rb\") as f:\n",
    "        dataset.extend(pickle.load(f))\n",
    "\n",
    "GrooveIQStats = DataStats()\n",
    "GrooveIQStats.set_name(name = name)\n",
    "\n",
    "for datapoint in tqdm(dataset):\n",
    "    GrooveIQStats.accumulate_dict(datapoint)\n",
    "\n",
    "note_loss_per_style = {key: [] for key in GrooveIQStats.style_map.keys()}\n",
    "\n",
    "for sample in tqdm(GrooveIQStats.all_samples):\n",
    "    _, stats = sample.feature.to_fixed_grid(steps_per_quarter=4)\n",
    "    note_loss_per_style[sample.style].append(stats['note_loss']/stats['total_notes'])\n",
    "\n",
    "for style, losses in note_loss_per_style.items():\n",
    "    print(f\"{style:30}: mean={np.mean(losses):.3f}, std={np.std(losses):.3f}\")\n",
    "\n",
    "plot_violin_distribution(note_loss_per_style, group_label='Style', feature=\"Note Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83519799",
   "metadata": {},
   "source": [
    "## Model Quantitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28940ee",
   "metadata": {},
   "source": [
    "### Load Eval Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load samples from pickle\n",
    "with open(\"dataset/serialized/eval_samples.pkl\", \"rb\") as f:\n",
    "    samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74585c63",
   "metadata": {},
   "source": [
    "### Load Checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a938d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_l1_causal    = load_checkpoint('expts/giq_exp1_l1_causal', 'checkpoint-ep4-model')\n",
    "model2_l1_ncausal   = load_checkpoint('expts/giq_exp2_l1_noncausal', 'checkpoint-ep4-model')\n",
    "model3_grp_causal   = load_checkpoint('expts/giq_exp3_group_causal', 'checkpoint-ep4-model')\n",
    "model4_grp_ncausal  = load_checkpoint('expts/giq_exp4_group_noncausal', 'checkpoint-ep4-model')\n",
    "model5_heur_causal  = load_checkpoint('expts/giq_exp5_heur_causal', 'checkpoint-ep4-model')\n",
    "model6_heur_ncausal = load_checkpoint('expts/giq_exp6_heur_noncausal', 'checkpoint-ep4-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4280583",
   "metadata": {},
   "source": [
    "### Eval Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9bbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_alignment_map = eval_alignment(samples, model1_l1_causal, is_heuristic=True)\n",
    "model2_alignment_map = eval_alignment(samples, model2_l1_ncausal, is_heuristic=True)\n",
    "model3_alignment_map = eval_alignment(samples, model3_grp_causal, is_heuristic=True)\n",
    "model4_alignment_map = eval_alignment(samples, model4_grp_ncausal, is_heuristic=True)\n",
    "model5_alignment_map = eval_alignment(samples, model5_heur_causal, is_heuristic=True)\n",
    "model6_alignment_map = eval_alignment(samples, model6_heur_ncausal, is_heuristic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model 1 Alignment Map: {\"\".join(f'\\n\\t{key}: Mean: {np.mean(model1_alignment_map[key])}, Std: {np.std(model1_alignment_map[key])}' for key in remove_nan(model1_alignment_map).keys())}\")\n",
    "print(f\"Model 2 Alignment Map: {\"\".join(f'\\n\\t{key}: Mean: {np.mean(model2_alignment_map[key])}, Std: {np.std(model2_alignment_map[key])}' for key in remove_nan(model2_alignment_map).keys())}\")\n",
    "print(f\"Model 3 Alignment Map: {\"\".join(f'\\n\\t{key}: Mean: {np.mean(model3_alignment_map[key])}, Std: {np.std(model3_alignment_map[key])}' for key in remove_nan(model3_alignment_map).keys())}\")\n",
    "print(f\"Model 4 Alignment Map: {\"\".join(f'\\n\\t{key}: Mean: {np.mean(model4_alignment_map[key])}, Std: {np.std(model4_alignment_map[key])}' for key in remove_nan(model4_alignment_map).keys())}\")\n",
    "print(f\"Model 5 Alignment Map: {\"\".join(f'\\n\\t{key}: Mean: {np.mean(model5_alignment_map[key])}, Std: {np.std(model5_alignment_map[key])}' for key in remove_nan(model5_alignment_map).keys())}\")\n",
    "print(f\"Model 6 Alignment Map: {\"\".join(f'\\n\\t{key}: Mean: {np.mean(model6_alignment_map[key])}, Std: {np.std(model6_alignment_map[key])}' for key in remove_nan(model6_alignment_map).keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_corrs_map = {\n",
    "    \"Model 1\" : model1_alignment_map['hit_corrs'],\n",
    "    \"Model 2\" : model2_alignment_map['hit_corrs'],\n",
    "    \"Model 3\" : model3_alignment_map['hit_corrs'],\n",
    "    \"Model 4\" : model4_alignment_map['hit_corrs'],\n",
    "    \"Model 5\" : model5_alignment_map['hit_corrs'],\n",
    "    \"Model 6\" : model6_alignment_map['hit_corrs']\n",
    "}\n",
    "\n",
    "peak_lags_map = {\n",
    "    \"Model 1\" : model1_alignment_map['peak_lags'],\n",
    "    \"Model 2\" : model2_alignment_map['peak_lags'],\n",
    "    \"Model 3\" : model3_alignment_map['peak_lags'],\n",
    "    \"Model 4\" : model4_alignment_map['peak_lags'],\n",
    "    \"Model 5\" : model5_alignment_map['peak_lags'],\n",
    "    \"Model 6\" : model6_alignment_map['peak_lags']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ece168",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_distribution(hit_corrs_map, group_label=\"Model\", feature=\"Hit Density Correlation\", title_fontsize=18, label_fontsize=16, tick_fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_distribution(peak_lags_map, group_label='Model', feature='Temporal Cross-Correlation Lag', title_fontsize=18, label_fontsize=16, tick_fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98832192",
   "metadata": {},
   "source": [
    "### Evaluate Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_samples_features = {\n",
    "        'total_hits' : [],\n",
    "        'total_density' : [],\n",
    "        'total_complexity' : [],\n",
    "        'total_average_intensity' : [],\n",
    "        'lowness' : [],\n",
    "        'midness' : [],\n",
    "        'highness' : [],\n",
    "        'combined_syncopation' : [],\n",
    "        'polyphonic_syncopation' : [],\n",
    "        'laidbackness' : [],\n",
    "        'swingness' : [],\n",
    "        'timing_accuracy' : [],\n",
    "}\n",
    "\n",
    "for sample in tqdm(samples):\n",
    "    sample, grid, button_hvo = sample['sample'], sample['grid'], sample['button_hvo']\n",
    "    gt_drum_feature = sample.feature\n",
    "    gt_descriptors = FeatureDescriptors(gt_drum_feature)\n",
    "    total_hits = torch.sum(grid[:, :, 0] > 0)\n",
    "    eval_samples_features['total_hits'].append(int(total_hits.item()))\n",
    "    for key in eval_samples_features.keys():\n",
    "        if key == 'total_hits':\n",
    "            continue\n",
    "        eval_samples_features[key].append(gt_descriptors.descriptors[key])\n",
    "\n",
    "print(f\"Ground Truth: {' '.join([f'\\n\\t{key}: {np.mean(eval_samples_features[key])}' for key in eval_samples_features.keys()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1be7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_feature_map = eval_features(samples, model1_l1_causal, is_heuristic=True)\n",
    "model2_feature_map = eval_features(samples, model2_l1_ncausal, is_heuristic=True)\n",
    "model3_feature_map = eval_features(samples, model3_grp_causal, is_heuristic=True)\n",
    "model4_feature_map = eval_features(samples, model4_grp_ncausal, is_heuristic=True)\n",
    "model5_feature_map = eval_features(samples, model5_heur_causal, is_heuristic=True)\n",
    "model6_feature_map = eval_features(samples, model6_heur_ncausal, is_heuristic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a87bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model 1: {' '.join([f'\\n\\t{key}: {np.mean(model1_feature_map[key])}' for key in model1_feature_map.keys()])}\")\n",
    "print(f\"Model 2: {' '.join([f'\\n\\t{key}: {np.mean(model2_feature_map[key])}' for key in model2_feature_map.keys()])}\")\n",
    "print(f\"Model 3: {' '.join([f'\\n\\t{key}: {np.mean(model3_feature_map[key])}' for key in model3_feature_map.keys()])}\")\n",
    "print(f\"Model 4: {' '.join([f'\\n\\t{key}: {np.mean(model4_feature_map[key])}' for key in model4_feature_map.keys()])}\")\n",
    "print(f\"Model 5: {' '.join([f'\\n\\t{key}: {np.mean(model5_feature_map[key])}' for key in model5_feature_map.keys()])}\")\n",
    "print(f\"Model 6: {' '.join([f'\\n\\t{key}: {np.mean(model6_feature_map[key])}' for key in model6_feature_map.keys()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems = {\n",
    "    'Model 1' : model1_feature_map,\n",
    "    'Model 2' : model2_feature_map,\n",
    "    'Model 3' : model3_feature_map,\n",
    "    'Model 4' : model4_feature_map,\n",
    "    'Model 5' : model5_feature_map,\n",
    "    'Model 6' : model6_feature_map\n",
    "}\n",
    "\n",
    "baseline_dict = eval_samples_features\n",
    "\n",
    "dists = compute_per_feature_distances(systems, baseline_dict)\n",
    "plot_kld_oa_per_feature(dists, system_names=list(systems.keys()), feature_title_fontsize=20, axis_label_fontsize=20, tick_fontsize=10, legend_fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hits_map = {\n",
    "    \"Ground Truth\" : eval_samples_features['total_hits'],\n",
    "    \"Model 1\" : model1_feature_map['total_hits'],\n",
    "    \"Model 2\" : model2_feature_map['total_hits'],\n",
    "    \"Model 3\" : model3_feature_map['total_hits'],\n",
    "    \"Model 4\" : model4_feature_map['total_hits'],\n",
    "    \"Model 5\" : model5_feature_map['total_hits'],\n",
    "    \"Model 6\" : model6_feature_map['total_hits']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_distribution_wrt_gt(total_hits_map, feature=\"Total Hits\", title_fontsize=18, label_fontsize=16, tick_fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e73598",
   "metadata": {},
   "source": [
    "### Evaluate Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7bdf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_recons_map = eval_recons(samples, model1_l1_causal, is_heuristic=False)\n",
    "model2_recons_map = eval_recons(samples, model2_l1_ncausal, is_heuristic=False)\n",
    "model3_recons_map = eval_recons(samples, model3_grp_causal, is_heuristic=False)\n",
    "model4_recons_map = eval_recons(samples, model4_grp_ncausal, is_heuristic=False)\n",
    "model5_recons_map = eval_recons(samples, model5_heur_causal, is_heuristic=True)\n",
    "model6_recons_map = eval_recons(samples, model6_heur_ncausal, is_heuristic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model 1: hit_ppv: {np.mean(model1_recons_map['hit_ppv']) :.2f}, hit_tpr: {np.mean(model1_recons_map['hit_tpr']) :.2f}, hit_f1: {np.mean(model1_recons_map['hit_f1']) :.2f}, velocity_mae: {np.mean(model1_recons_map['velocity_mae']) :.2f}, offset_mae: {np.mean(model1_recons_map['offset_mae']) :.2f}\")\n",
    "print(f\"Model 2: hit_ppv: {np.mean(model2_recons_map['hit_ppv']) :.2f}, hit_tpr: {np.mean(model2_recons_map['hit_tpr']) :.2f}, hit_f1: {np.mean(model2_recons_map['hit_f1']) :.2f}, velocity_mae: {np.mean(model2_recons_map['velocity_mae']) :.2f}, offset_mae: {np.mean(model2_recons_map['offset_mae']) :.2f}\")\n",
    "print(f\"Model 3: hit_ppv: {np.mean(model3_recons_map['hit_ppv']) :.2f}, hit_tpr: {np.mean(model3_recons_map['hit_tpr']) :.2f}, hit_f1: {np.mean(model3_recons_map['hit_f1']) :.2f}, velocity_mae: {np.mean(model3_recons_map['velocity_mae']) :.2f}, offset_mae: {np.mean(model3_recons_map['offset_mae']) :.2f}\")\n",
    "print(f\"Model 4: hit_ppv: {np.mean(model4_recons_map['hit_ppv']) :.2f}, hit_tpr: {np.mean(model4_recons_map['hit_tpr']) :.2f}, hit_f1: {np.mean(model4_recons_map['hit_f1']) :.2f}, velocity_mae: {np.mean(model4_recons_map['velocity_mae']) :.2f}, offset_mae: {np.mean(model4_recons_map['offset_mae']) :.2f}\")\n",
    "print(f\"Model 5: hit_ppv: {np.mean(model5_recons_map['hit_ppv']) :.2f}, hit_tpr: {np.mean(model5_recons_map['hit_tpr']) :.2f}, hit_f1: {np.mean(model5_recons_map['hit_f1']) :.2f}, velocity_mae: {np.mean(model5_recons_map['velocity_mae']) :.2f}, offset_mae: {np.mean(model5_recons_map['offset_mae']) :.2f}\")\n",
    "print(f\"Model 6: hit_ppv: {np.mean(model6_recons_map['hit_ppv']) :.2f}, hit_tpr: {np.mean(model6_recons_map['hit_tpr']) :.2f}, hit_f1: {np.mean(model6_recons_map['hit_f1']) :.2f}, velocity_mae: {np.mean(model6_recons_map['velocity_mae']) :.2f}, offset_mae: {np.mean(model6_recons_map['offset_mae']) :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_ppv_map = {\n",
    "    \"Model 1\" : model1_recons_map['hit_ppv'],\n",
    "    \"Model 2\" : model2_recons_map['hit_ppv'],\n",
    "    \"Model 3\" : model3_recons_map['hit_ppv'],\n",
    "    \"Model 4\" : model4_recons_map['hit_ppv'],\n",
    "    \"Model 5\" : model5_recons_map['hit_ppv'],\n",
    "    \"Model 6\" : model6_recons_map['hit_ppv']\n",
    "}\n",
    "\n",
    "plot_violin_distribution(hit_ppv_map, group_label='Model', feature=\"PPV Hits\", title_fontsize=18, label_fontsize=16, tick_fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_tpr_map = {\n",
    "    \"Model 1\" : model1_recons_map['hit_tpr'],\n",
    "    \"Model 2\" : model2_recons_map['hit_tpr'],\n",
    "    \"Model 3\" : model3_recons_map['hit_tpr'],\n",
    "    \"Model 4\" : model4_recons_map['hit_tpr'],\n",
    "    \"Model 5\" : model5_recons_map['hit_tpr'],\n",
    "    \"Model 6\" : model6_recons_map['hit_tpr']\n",
    "}\n",
    "\n",
    "plot_violin_distribution(hit_tpr_map, group_label='Model', feature=\"TPR Hits\", title_fontsize=18, label_fontsize=16, tick_fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_mae_map = {\n",
    "    \"Model 1\" : model1_recons_map['velocity_mae'],\n",
    "    \"Model 2\" : model2_recons_map['velocity_mae'],\n",
    "    \"Model 3\" : model3_recons_map['velocity_mae'],\n",
    "    \"Model 4\" : model4_recons_map['velocity_mae'],\n",
    "    \"Model 5\" : model5_recons_map['velocity_mae'],\n",
    "    \"Model 6\" : model6_recons_map['velocity_mae']\n",
    "}\n",
    "\n",
    "plot_violin_distribution(velocity_mae_map, group_label='Model', feature=\"Velocity MAE\", title_fontsize=18, label_fontsize=16, tick_fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e611145",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mae_map = {\n",
    "    \"Model 1\" : model1_recons_map['offset_mae'],\n",
    "    \"Model 2\" : model2_recons_map['offset_mae'],\n",
    "    \"Model 3\" : model3_recons_map['offset_mae'],\n",
    "    \"Model 4\" : model4_recons_map['offset_mae'],\n",
    "    \"Model 5\" : model5_recons_map['offset_mae'],\n",
    "    \"Model 6\" : model6_recons_map['offset_mae']\n",
    "}\n",
    "\n",
    "plot_violin_distribution(offset_mae_map, group_label='Model', feature=\"Offset MAE\", title_fontsize=18, label_fontsize=16, tick_fontsize=18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
