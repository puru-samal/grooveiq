{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "from torchinfo import summary\n",
    "from models import GrooveIQ\n",
    "from data import CANONICAL_DRUM_MAP, SampleData, DrumMIDIFeature, DrumMIDIDataset\n",
    "from copy import deepcopy\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model from expts/giq_exp5_heur_causal/checkpoints/checkpoint-ep1-model.pth: <All keys matched successfully>\n",
      "Loading dataset from: dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_test.pkl...\n",
      "Processing 38370 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accumulating:: 100%|██████████| 38370/38370 [00:06<00:00, 6091.79sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples due to errors.\n",
      "Loaded and processed 38370 samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======= Set Experiment Path Here =======\n",
    "EXPT_PATH = \"expts/giq_exp5_heur_causal\"\n",
    "DATASET_PATH = \"dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_test.pkl\"\n",
    "CHECKPOINT_PATH = os.path.join(EXPT_PATH, \"checkpoints\", \"checkpoint-ep1-model.pth\")\n",
    "\n",
    "# ======= Config =======\n",
    "config_path = os.path.join(EXPT_PATH, \"config.yaml\")\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ======= Audio Save Directory =======\n",
    "audio_save_dir = os.path.join(EXPT_PATH, \"_renders\")\n",
    "os.makedirs(audio_save_dir, exist_ok=True)\n",
    "\n",
    "# ======= Parameters =======\n",
    "# Mapping for button sequence\n",
    "fixed_grid_drum_mapping = {pitch: [i] for i, pitch in enumerate(CANONICAL_DRUM_MAP.keys())}\n",
    "MAX_LENGTH = 33\n",
    "E = 9 # Number of drum instruments\n",
    "M = 3 # Number of steps per quarter\n",
    "\n",
    "# ======= Model =======\n",
    "model_config = config[\"model\"]\n",
    "model_config.update(\n",
    "    T=MAX_LENGTH,\n",
    "    E=E,\n",
    "    M=M\n",
    ")\n",
    "\n",
    "# ======= Load Model =======\n",
    "model = GrooveIQ(**model_config)\n",
    "input_size = [(4, 33, 9, 3)]\n",
    "print(f\"Loading model from {CHECKPOINT_PATH}: \", end=\"\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "print(model.load_state_dict(checkpoint['model_state_dict'], strict=True))\n",
    "summary(model, input_size=input_size, device = device)\n",
    "\n",
    "# ======= Load Dataset =======\n",
    "test_dataset = DrumMIDIDataset(\n",
    "    path     = DATASET_PATH,\n",
    "    num_bars = config[\"data\"][\"num_bars\"],\n",
    "    feature_type = config[\"data\"][\"feature_type\"],\n",
    "    steps_per_quarter = config[\"data\"][\"steps_per_quarter\"],\n",
    "    subset   = 1.0,\n",
    "    aug_config = config[\"data\"][\"aug_config\"],\n",
    "    calc_desc = config[\"data\"][\"calc_desc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model : GrooveIQ, sample : SampleData, button_hits : torch.Tensor, grid : torch.Tensor, device : str = \"cpu\", threshold : float = 0.85):\n",
    "    \"\"\"\n",
    "    Inference function for GrooveIQ2 model.\n",
    "\n",
    "    Args:\n",
    "        model (GrooveIQ2): GrooveIQ2 model\n",
    "        sample (SampleData): SampleData object\n",
    "        button_hits (Tensor): Button hits tensor used to create button_embed. If None, button_hits is created from button_repr.\n",
    "                              Shape: (1, T, num_buttons)\n",
    "        \n",
    "        grid (Tensor): Grid tensor used to create z. If None, z is sampled from prior.\n",
    "                       Shape: (1, T, E, M)\n",
    "        threshold (float, optional): Threshold for hit probability. Defaults to 0.85.\n",
    "        device (str, optional): Device to use. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        DrumMIDIFeature: Generated feature\n",
    "        DrumMIDIFeature: Button feature\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode grid if provided\n",
    "        encoded, button_repr = None, None\n",
    "        if grid is not None:\n",
    "            encoded, button_repr = model.encode(grid)\n",
    "\n",
    "        # Make button_embed from either user-provided button_hits or learned button_repr\n",
    "        # depending on the model's configuration\n",
    "        # If both are available, button_hits is used\n",
    "        if button_hits is None:\n",
    "            if button_repr is None:\n",
    "                raise ValueError(\"button_repr is None. Either provide button_hits or grid.\")\n",
    "            button_hits = model.make_button_hits(button_repr)\n",
    "            \n",
    "        button_embed = model.make_button_embed(button_hits)\n",
    "        if encoded is None:\n",
    "            z, _, _ = model.make_z_prior(button_embed)\n",
    "        else:\n",
    "            z, _, _ = model.make_z_post(button_embed, encoded)\n",
    "\n",
    "        generated_grid, _ = model.generate(button_embed, z, max_steps=MAX_LENGTH, threshold=threshold)\n",
    "        generated_grid = generated_grid[:, 1:, :, :] # Drop SOS token\n",
    "        generated_sample = sample.from_fixed_grid(generated_grid.squeeze(0), steps_per_quarter=4)\n",
    "        generated_feature = generated_sample.feature\n",
    "\n",
    "        button_hvo = torch.cat(\n",
    "                [\n",
    "                    button_hits.unsqueeze(-1), \n",
    "                    torch.ones_like(button_hits).unsqueeze(-1).repeat(1, 1, 1,1) * 0.8,  # 0.8 : velocity\n",
    "                    torch.zeros_like(button_hits).unsqueeze(-1).repeat(1, 1, 1, 1),      # 0 : offset\n",
    "                ], dim=-1) # (1, T, num_buttons, M)\n",
    "        button_hvo = button_hvo.squeeze(0) # (T, num_buttons, M)\n",
    "        button_feature = generated_sample.feature.from_button_hvo(button_hvo, steps_per_quarter=4)\n",
    "\n",
    "    return generated_feature, button_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Playback\n",
    "Ground Truth Drum Sequence -> Button Sequence -> Reconstructed Drum Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= User-defined gap (seconds) ========\n",
    "gap_sec = 2.0  # <-- set your gap duration here\n",
    "sample_rate = 44100  # or whatever your playback function uses\n",
    "sample_z = False\n",
    "combined_audio = []\n",
    "num_samples = 4\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # ======= Pick random test sample =======\n",
    "    random_idx = np.random.randint(0, len(test_dataset))\n",
    "    sample, grid, button_hvo, desc_label = test_dataset[random_idx]\n",
    "    sample.feature.play()\n",
    "\n",
    "    grid = grid.unsqueeze(0)\n",
    "    if button_hvo is not None:\n",
    "        button_hits = button_hvo[:, :, 0].unsqueeze(0) # (1, T, num_buttons)\n",
    "    else:\n",
    "        button_hits = None\n",
    "\n",
    "    generated_feature, button_feature = inference(model, sample, button_hits, None, device=\"cpu\", threshold=0.85)\n",
    "\n",
    "    # ======= Play generated audio =======\n",
    "    button_audio = button_feature.play_button_hvo(button_feature)\n",
    "    generated_audio = generated_feature.play()\n",
    "\n",
    "    # ======= Create silence gap =======\n",
    "    gap_samples = int(sample_rate * gap_sec)\n",
    "    silence_gap = np.zeros((gap_samples, 2), dtype=button_audio.dtype)\n",
    "\n",
    "    # ======= Concatenate all =======\n",
    "    if i == 0:\n",
    "        combined_audio = np.concatenate([button_audio, silence_gap, generated_audio], axis=0)\n",
    "    else:\n",
    "        combined_audio = np.concatenate([combined_audio, button_audio, silence_gap, generated_audio], axis=0)\n",
    "\n",
    "# ======= Save to file =======\n",
    "output_path = os.path.join(audio_save_dir, f\"combined_output_{random_idx}.wav\")\n",
    "sf.write(output_path, combined_audio, sample_rate)\n",
    "print(f\"Saved combined audio to {output_path}\")\n",
    "\n",
    "# ======= Play in notebook =======\n",
    "Audio(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_map          = {style: [] for style in test_dataset.data_stats.style_map.keys() if style != \"unknown\"}\n",
    "samples_per_style  = 100\n",
    "styles_complete    = set()\n",
    "errors             = 0\n",
    "\n",
    "pbar = tqdm(total=len(test_dataset), desc=\"Collecting Encoded Vectors\")\n",
    "for i in range(len(test_dataset)):\n",
    "    if len(styles_complete) == len(style_map):\n",
    "        break\n",
    "    try:\n",
    "        sample, grid, button_hvo, desc_label = test_dataset[i]\n",
    "\n",
    "        if sample.style == \"unknown\":\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        if len(style_map[sample.style]) >= samples_per_style:\n",
    "            styles_complete.add(sample.style)\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        style_map[sample.style].append(i)\n",
    "\n",
    "        if len(style_map[sample.style]) >= samples_per_style:\n",
    "            styles_complete.add(sample.style)\n",
    "\n",
    "        pbar.update(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding grid {i}: {e}\")\n",
    "        errors += 1\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "print(f\"Total errors: {errors}\")\n",
    "for key in style_map.keys():\n",
    "    print(f\"Style: {key}, #Samples: {len(style_map[key])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= User-defined gap (seconds) ========\n",
    "gap_sec = 2.0  # <-- set your gap duration here\n",
    "sample_rate = 44100  # or whatever your playback function uses\n",
    "combined_audio = []\n",
    "num_samples = 1\n",
    "num_styles  = 4\n",
    "to_plots  = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # ======= Pick random test sample =======\n",
    "    random_idx = np.random.randint(0, len(test_dataset))\n",
    "    original_sample, _, button_hvo, _ = test_dataset[random_idx]\n",
    "    # ======= Pick random style clusters =======\n",
    "    for j in range(num_styles):\n",
    "        # Pick random style\n",
    "        random_style = np.random.choice(list(style_map.keys()))\n",
    "        print(f\"Random style: {random_style}\")\n",
    "        random_style_idx = np.random.choice(style_map[random_style])\n",
    "        print(f\"Random style index: {random_style_idx}\")\n",
    "        style_sample, style_grid, _, _ = test_dataset[random_style_idx]\n",
    "\n",
    "        grid = grid.unsqueeze(0)\n",
    "        if button_hvo is not None:\n",
    "            control_hits = button_hvo[:, :, 0].unsqueeze(0) # (1, T, num_buttons)\n",
    "        else:\n",
    "            control_hits = None\n",
    "\n",
    "        if j == 0:\n",
    "            generated_feature, control_feature = inference(model, sample, control_hits, None, device=\"cpu\", threshold=0.85)\n",
    "        else:\n",
    "            generated_feature, control_feature = inference(model, sample, control_hits, style_grid.unsqueeze(0), device=\"cpu\", threshold=0.85)\n",
    "\n",
    "        if j == 0:\n",
    "            to_plots.append(control_feature)\n",
    "        to_plots.append(generated_feature)\n",
    "\n",
    "        # ======= Play generated audio =======\n",
    "        if j == 0:\n",
    "            control_audio = control_feature.play_button_hvo(control_feature)\n",
    "            #style_audio = style_sample.feature.play()\n",
    "        generated_audio = generated_feature.play()\n",
    "\n",
    "        # ======= Create silence gap =======\n",
    "        gap_samples = int(sample_rate * gap_sec)\n",
    "        silence_gap = np.zeros((gap_samples, 2), dtype=control_audio.dtype)\n",
    "\n",
    "        # ======= Concatenate all =======\n",
    "        if j == 0:\n",
    "            combined_audio.append(control_audio)\n",
    "        combined_audio.append(generated_audio)\n",
    "\n",
    "# ======= Save to file =======\n",
    "for i, audio in enumerate(combined_audio):\n",
    "    output_path = os.path.join(audio_save_dir, f\"style_{i}_output_{random_idx}.wav\")\n",
    "    sf.write(output_path, audio, sample_rate)\n",
    "    print(f\"Saved combined audio to {output_path}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(len(to_plots), 1, figsize=(15, 15))\n",
    "DrumMIDIFeature._grid_plot(\n",
    "        to_plots[0].to_button_hvo(steps_per_quarter=4, num_buttons=2),\n",
    "        ax=axes[0],\n",
    "        title=f\"Control Sequence\",\n",
    "        xlabel=\"Time Step\",\n",
    "        ylabel=\"Control Class\"\n",
    ")\n",
    "\n",
    "for i in range(1, len(to_plots)):\n",
    "    to_plots[i].fixed_grid_plot(ax=axes[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# ======= Play in notebook =======\n",
    "#Audio(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def mask_beats_structured(\n",
    "    button_hvo_base: torch.Tensor,\n",
    "    steps_per_beat: int = 4,\n",
    "    keep_beats: list = [0, 2],\n",
    "    total_beats: int = 4\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Masks out full beats except those specified in `keep_beats`.\n",
    "\n",
    "    Args:\n",
    "        button_hvo_base (Tensor): shape (B, T, num_buttons, M)\n",
    "        steps_per_beat (int): number of time steps per beat (default 4 = 16th-note grid at 4/4 time)\n",
    "        keep_beats (list): indices of beats to retain (e.g., [0, 2] = keep beat 1 and 3)\n",
    "        total_beats (int): total number of beats in the bar (default 4 for 4/4)\n",
    "\n",
    "    Returns:\n",
    "        Tensor: masked button_hvo_base of same shape\n",
    "    \"\"\"\n",
    "    B, T, num_buttons, M = button_hvo_base.shape\n",
    "    steps_per_bar = steps_per_beat * total_beats\n",
    "    num_bars = T // steps_per_bar\n",
    "    mask = torch.zeros((T,), device=button_hvo_base.device)\n",
    "\n",
    "    for bar_idx in range(num_bars):\n",
    "        bar_start = bar_idx * steps_per_bar\n",
    "        for beat in keep_beats:\n",
    "            start = bar_start + beat * steps_per_beat\n",
    "            end = start + steps_per_beat\n",
    "            if end <= T:\n",
    "                mask[start:end] = 1.0\n",
    "\n",
    "    # Handle leftover steps at the end if T is not a multiple of steps_per_bar\n",
    "    leftover = T % steps_per_bar\n",
    "    if leftover > 0:\n",
    "        for beat in keep_beats:\n",
    "            start = num_bars * steps_per_bar + beat * steps_per_beat\n",
    "            end = start + steps_per_beat\n",
    "            if start < T:\n",
    "                mask[start:min(end, T)] = 1.0\n",
    "\n",
    "    # Broadcast to match shape\n",
    "    mask = mask.view(1, T, 1, 1)\n",
    "    return button_hvo_base * mask\n",
    "\n",
    "\n",
    "def shift_button_sequence(button_hvo_base, shift_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Randomly shifts each time step's button activation forward or backward within the shift_range.\n",
    "    Wraps at boundaries (circular).\n",
    "    \n",
    "    Args:\n",
    "        button_hvo_base: (1, T, B, M) tensor\n",
    "        shift_range: (min_shift, max_shift), inclusive\n",
    "    \"\"\"\n",
    "    B, T, B_, M = button_hvo_base.shape\n",
    "    shifted = torch.zeros_like(button_hvo_base)\n",
    "\n",
    "    for t in range(T):\n",
    "        shift = random.randint(*shift_range)\n",
    "        t_new = (t + shift) % T\n",
    "        shifted[:, t_new] += button_hvo_base[:, t]\n",
    "    \n",
    "    return shifted.clamp(0, 1.0)  # ensure values stay valid\n",
    "\n",
    "\n",
    "def mask_random_within_regions(button_hvo_base, steps_per_beat=4, beats_to_keep=[0, 2], total_beats=4, retain_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Keeps given beats but randomly drops hits within the beat window.\n",
    "    \n",
    "    Args:\n",
    "        button_hvo_base: (1, T, B, M) tensor\n",
    "        steps_per_beat: how many steps per beat (usually 4 for 16th grid)\n",
    "        beats_to_keep: list of beat indices to retain\n",
    "        total_beats: total number of beats in the bar (default 4 for 4/4)\n",
    "        retain_ratio: proportion of hits to keep within kept beats\n",
    "    \"\"\"\n",
    "    B, T, num_buttons, M = button_hvo_base.shape\n",
    "    steps_per_bar = steps_per_beat * total_beats\n",
    "    num_bars = T // steps_per_bar\n",
    "    mask = torch.zeros((T,), device=button_hvo_base.device)\n",
    "\n",
    "    for bar_idx in range(num_bars + 1):  # include partial last bar\n",
    "        bar_start = bar_idx * steps_per_bar\n",
    "        for beat in range(total_beats):\n",
    "            beat_start = bar_start + beat * steps_per_beat\n",
    "            beat_end = beat_start + steps_per_beat\n",
    "\n",
    "            if beat_start >= T:\n",
    "                break\n",
    "\n",
    "            if torch.rand(1).item() < retain_ratio:\n",
    "                mask[beat_start:min(beat_end, T)] = 1.0\n",
    "\n",
    "    mask = mask.view(1, T, 1, 1)  # broadcast to match button_hvo shape\n",
    "    return button_hvo_base * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "\n",
    "# ======= Pick random result =======\n",
    "random_idx = np.random.randint(0, len(test_dataset))\n",
    "sample_rate = 44100\n",
    "gap_sec = 2.0\n",
    "num_expts = 4\n",
    "\n",
    "# ======= Masking =======\n",
    "total_beats = 4\n",
    "steps_per_beat = 5\n",
    "mask_progression = [\n",
    "    [0, 1, 2, 3], # No mask\n",
    "    [0, 2],       # Strong beat only\n",
    "    [1, 3],       # Off beat only\n",
    "    [0, 1],       # Downbeat and upbeat\n",
    "    [0],          # Downbeat only\n",
    "]  # Number of beats to keep\n",
    "threshold = 0.85\n",
    "\n",
    "data = test_dataset[random_idx]\n",
    "button_audios = []\n",
    "generated_audios = []\n",
    "button_plots = []\n",
    "generated_plots = []\n",
    "\n",
    "for i in range(num_expts):\n",
    "    sample, grid, button_hvo, desc_label = data\n",
    "    button_hvo_mask = mask_beats_structured(button_hvo.unsqueeze(0), steps_per_beat, mask_progression[i], total_beats)\n",
    "\n",
    "    encoded, button_repr = model.encode(grid.unsqueeze(0))\n",
    "    button_hits = button_hvo_mask[:, :, :, 0] # (B, T, num_buttons)\n",
    "    button_embed = model.make_button_embed(button_hits)\n",
    "    z_post = model.sample_z_from_button_embed(button_embed)\n",
    "    generated_grids, hit_probs = model.generate(button_embed, z_post, max_steps=33, threshold=0.85)\n",
    "    generated_grids = generated_grids[:, 1:, :, :] # Drop SOS token\n",
    "    generated_grids = generated_grids.squeeze(0)\n",
    "\n",
    "    generated_sample = sample.from_fixed_grid(generated_grids, steps_per_quarter=4)\n",
    "    generated_feature = generated_sample.feature\n",
    "    button_hvo_tmp = torch.cat(\n",
    "                        [\n",
    "                            button_hits.unsqueeze(-1), \n",
    "                            torch.ones_like(button_hits).unsqueeze(-1).repeat(1, 1, 1,1) * 0.8,  # 0.8 : velocity\n",
    "                            torch.zeros_like(button_hits).unsqueeze(-1).repeat(1, 1, 1, 1),      # 0 : offset\n",
    "                        ], dim=-1) # (1, T, num_buttons, M)\n",
    "    button_hvo_tmp = button_hvo_tmp.squeeze(0) # (T, num_buttons, M)\n",
    "    button_feature = generated_sample.feature.from_button_hvo(button_hvo_tmp, steps_per_quarter=4)\n",
    "\n",
    "    button_plots.append(button_feature)\n",
    "    generated_plots.append(generated_feature)\n",
    "\n",
    "    button_audios.append(button_feature.play_button_hvo(button_feature))\n",
    "    generated_audios.append(generated_feature.play())\n",
    "\n",
    "fig, axes = plt.subplots(len(button_plots), 2, figsize=(15, 15))\n",
    "\n",
    "for i, (button_audio, generated_audio) in enumerate(zip(button_audios, generated_audios)):\n",
    "    button_output_path = os.path.join(audio_save_dir, f\"mask_{i}_button_output_{random_idx}.wav\")\n",
    "    generated_output_path = os.path.join(audio_save_dir, f\"mask_{i}_generated_output_{random_idx}.wav\")\n",
    "\n",
    "    sf.write(button_output_path, button_audio, sample_rate)\n",
    "    sf.write(generated_output_path, generated_audio, sample_rate)\n",
    "\n",
    "for i in range(len(button_plots)):\n",
    "    DrumMIDIFeature._grid_plot(\n",
    "        button_plots[i].to_button_hvo(steps_per_quarter=4, num_buttons=2),\n",
    "        ax=axes[i, 0],\n",
    "        title=f\"Control Sequence\",\n",
    "        xlabel=\"Time Step\",\n",
    "        ylabel=\"Control Class\"\n",
    "    )\n",
    "    generated_plots[i].fixed_grid_plot(ax=axes[i, 1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
