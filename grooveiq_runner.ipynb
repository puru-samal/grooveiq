{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from data import DrumMIDIDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from models import GrooveIQ\n",
    "from trainers import GrooveIQ_Trainer\n",
    "from utils import create_optimizer, create_scheduler, WeightScheduler\n",
    "import wandb\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile _configs/config_test.yaml\n",
    "\n",
    "name : \"Puru\"\n",
    "expt : \"Test\"\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:\n",
    "    train_path: \"dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_train.pkl\"\n",
    "    val_path: \"dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_val.pkl\"\n",
    "    test_path: \"dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_test.pkl\"\n",
    "    num_bars: 2\n",
    "    feature_type: \"fixed\"  # [fixed, flexible]\n",
    "    steps_per_quarter: 4\n",
    "    subset: 0.01             # Fraction of the dataset to load\n",
    "    num_workers: 0           # Number of workers for data loading\n",
    "    batch_size: 16           # Batch size\n",
    "    aug_config:\n",
    "        win_sizes:\n",
    "          - size: 2\n",
    "            prob: 0.75\n",
    "          - size: 3\n",
    "            prob: 0.125\n",
    "          - size: 4\n",
    "            prob: 0.125\n",
    "        velocity_range:\n",
    "          min: 0.45\n",
    "          max: 0.80\n",
    "        max_hits_per_win: 2\n",
    "        win_retain_prob: 1.0\n",
    "        num_buttons: 2\n",
    "        timing_jitter: 0.05\n",
    "        velocity_jitter: 0.05\n",
    "        miss_prob: 0.05\n",
    "        spurious_prob: 0.1\n",
    "    calc_desc: False\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model:\n",
    "    z_dim: 16\n",
    "    embed_dim: 32\n",
    "    encoder_depth: 1\n",
    "    encoder_heads: 1\n",
    "    decoder_depth: 1\n",
    "    decoder_heads: 1\n",
    "    num_buttons:   2\n",
    "    is_causal: True\n",
    "    p: 0.5 # Probability of using z_post instead of z_prior\n",
    "    button_penalty: 1 # 1 : L1, 2 : Group (L1 over T of L2(D))\n",
    "    button_dropout: 0.2 # Dropout probability for button hits\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  config_file                 : \"_configs/config_test.yaml\"\n",
    "  use_wandb                   : False   # Toggle wandb logging\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : True   # Resume an existing run (run_id != 'none')\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"thesis\" # wandb project to log to\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss:\n",
    "  pos_weight                  : 21.0 # Weight for positive class in hit loss\n",
    "  hit_penalty                 : 21.0 # Hit penalty for hit loss (21 for train set) \n",
    "  threshold                   : 0.5  # Threshold for hit prediction\n",
    "  recons_weight               : 1.0  # Weight for reconstruction loss\n",
    "  kld_weight                  : 0.01 # Weight for kld loss\n",
    "  distill_weight              : 0.01 # Weight for distill loss\n",
    "  button_penalty_weight       : 0.0  # Weight for button penalty loss\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adamw\" # Options: sgd, adam, adamw\n",
    "  lr: 0.0004    # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.000001\n",
    "\n",
    "  # Parameter groups\n",
    "  # You can add more param groups as you want and set their learning rates and patterns\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: []  # Will match all parameters containing \"encoder\"\n",
    "      lr: 0.0002    # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "    - name: ffn\n",
    "      patterns: []\n",
    "      lr: 0.0002  # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 15  # Maximum number of iterations\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 10    # Number of iterations for the first restart\n",
    "    T_mult: 10 # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0\n",
    "\n",
    "###### KL Weight Scheduler -----------------------------------------------------------------\n",
    "kl_weight_scheduler:\n",
    "  enabled: True\n",
    "  params:\n",
    "    weight_max: 0.2\n",
    "    total_epochs: 5\n",
    "    zero_epochs: 0\n",
    "    warmup_epochs: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_configs/config_test.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets / Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Datasets\n",
    "train_dataset = DrumMIDIDataset(\n",
    "    path     = config[\"data\"][\"train_path\"],\n",
    "    num_bars = config[\"data\"][\"num_bars\"],\n",
    "    feature_type = config[\"data\"][\"feature_type\"],\n",
    "    steps_per_quarter = config[\"data\"][\"steps_per_quarter\"],\n",
    "    subset   = config[\"data\"][\"subset\"],\n",
    "    aug_config = config[\"data\"][\"aug_config\"],\n",
    "    calc_desc = False\n",
    ")\n",
    "\n",
    "val_dataset  = DrumMIDIDataset(\n",
    "    path     = config[\"data\"][\"val_path\"],\n",
    "    num_bars = config[\"data\"][\"num_bars\"],\n",
    "    feature_type      = config[\"data\"][\"feature_type\"],\n",
    "    steps_per_quarter = config[\"data\"][\"steps_per_quarter\"],\n",
    "    subset   = config[\"data\"][\"subset\"],\n",
    "    aug_config = config[\"data\"][\"aug_config\"],\n",
    "    calc_desc = False\n",
    ")\n",
    "\n",
    "test_dataset = DrumMIDIDataset(\n",
    "    path     = config[\"data\"][\"test_path\"],\n",
    "    num_bars = config[\"data\"][\"num_bars\"],\n",
    "    feature_type      = config[\"data\"][\"feature_type\"],\n",
    "    steps_per_quarter = config[\"data\"][\"steps_per_quarter\"],\n",
    "    subset   = config[\"data\"][\"subset\"],\n",
    "    aug_config = config[\"data\"][\"aug_config\"],\n",
    "    calc_desc = False\n",
    ")\n",
    "\n",
    "## Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = config[\"data\"][\"batch_size\"],\n",
    "    num_workers = config[\"data\"][\"num_workers\"],\n",
    "    shuffle = True,\n",
    "    collate_fn = train_dataset.collate_fn,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = config[\"data\"][\"batch_size\"],\n",
    "    num_workers = config[\"data\"][\"num_workers\"],\n",
    "    shuffle = True,\n",
    "    collate_fn = val_dataset.collate_fn,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = config[\"data\"][\"batch_size\"],\n",
    "    num_workers = config[\"data\"][\"num_workers\"],\n",
    "    shuffle = True,\n",
    "    collate_fn = test_dataset.collate_fn,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "## Test a sample\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch size: {len(batch['samples'])}\")\n",
    "    print(f\"Grid shape: {batch['grid'].shape}\")\n",
    "    if batch['button_hvo'] is not None:\n",
    "        print(f\"Button HVO shape: {batch['button_hvo'].shape}\")\n",
    "    else:\n",
    "        print(\"No button HVO\")\n",
    "    if batch['labels'] is not None: \n",
    "        print(f\"Label shape: {batch['labels'].shape}\")\n",
    "    else:\n",
    "        print(\"No labels\")\n",
    "    grid = batch['grid']\n",
    "    random_idx = np.random.randint(len(batch['samples']))\n",
    "    sample = batch['samples'][random_idx]\n",
    "    sample.feature.play()\n",
    "    if batch['button_hvo'] is not None:\n",
    "        button_hvo = batch['button_hvo'][random_idx]\n",
    "        button_hvo_feature = sample.feature.from_button_hvo(button_hvo, steps_per_quarter=4)\n",
    "        button_hvo_feature.play_button_hvo(button_hvo_feature)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUARTERS_PER_BAR = 4 # 4/4 time signature\n",
    "MAX_LENGTH = config[\"data\"][\"num_bars\"] * NUM_QUARTERS_PER_BAR * config[\"data\"][\"steps_per_quarter\"] + 1\n",
    "print(f\"Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config[\"model\"]\n",
    "model_config.update(\n",
    "    T=MAX_LENGTH,\n",
    "    E=grid.shape[2],\n",
    "    M=grid.shape[3]\n",
    ")\n",
    "\n",
    "inputs = [grid]\n",
    "model = GrooveIQ(**model_config)\n",
    "summary(model, input_data = inputs, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GrooveIQ_Trainer(\n",
    "    model = model,\n",
    "    config = config,\n",
    "    run_name = config[\"expt\"],\n",
    "    config_file = config['training']['config_file'],\n",
    "    device = device\n",
    ")\n",
    "\n",
    "trainer.set_optimizer(\n",
    "    create_optimizer(\n",
    "        model=model,\n",
    "        opt_config=config['optimizer']\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer.set_scheduler(\n",
    "    create_scheduler(\n",
    "        optimizer=trainer.optimizer,\n",
    "        scheduler_config=config['scheduler'],\n",
    "        train_loader=train_loader,\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    "    )\n",
    ")\n",
    "\n",
    "if config['kl_weight_scheduler']['enabled']:\n",
    "    trainer.set_kl_weight_scheduler(\n",
    "        WeightScheduler(**config['kl_weight_scheduler']['params'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
