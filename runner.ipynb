{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'thesis' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Replace with your token and repo URL\n",
    "token = \"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\"\n",
    "!git clone https://{token}@github.com/puru-samal/thesis.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from data import DrumMIDIDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from models.GrooveIQ import GrooveIQ\n",
    "from trainers import GrooveIQ_Trainer\n",
    "from utils import create_optimizer, create_scheduler\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting _configs/config_test.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile _configs/config_test.yaml\n",
    "\n",
    "name : \"Puru\"\n",
    "expt : \"Test\"\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:\n",
    "    train_path: \"dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_train.pkl\"\n",
    "    val_path: \"dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_val.pkl\"\n",
    "    test_path: \"dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_test.pkl\"\n",
    "    num_bars: 2\n",
    "    feature_type: \"fixed\"  # [fixed, flexible]\n",
    "    steps_per_quarter: 4\n",
    "    subset: 0.01            # Fraction of the dataset to load\n",
    "    num_workers: 0         # Number of workers for data loading\n",
    "    batch_size: 16         # Batch size\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model:\n",
    "    embed_dim: 32\n",
    "    encoder_type: \"axial\" # [conv, temporal, spatial, mlp, axial]\n",
    "    encoder_depth: 1\n",
    "    encoder_heads: 1\n",
    "    decoder_type: \"transformer\" # [transformer, gru, mlp, conv]\n",
    "    decoder_depth: 1\n",
    "    decoder_heads: 1\n",
    "    num_buttons:   3\n",
    "    num_bins_velocity: 8\n",
    "    num_bins_offset:  16\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  config_file                 : \"_configs/config_test.yaml\"\n",
    "  use_wandb                   : False   # Toggle wandb logging\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : True   # Resume an existing run (run_id != 'none')\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"Set-Project-Name-Here\" # wandb project to log to\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss:\n",
    "  pos_weight                  : 21.0 # Positive weight for hit loss (21 for train set)\n",
    "  hit_penalty                 : 21.0 # Hit penalty for hit loss (2 for train set)\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adamw\" # Options: sgd, adam, adamw\n",
    "  lr: 0.0004    # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.000001\n",
    "\n",
    "  # Parameter groups\n",
    "  # You can add more param groups as you want and set their learning rates and patterns\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: []  # Will match all parameters containing \"encoder\"\n",
    "      lr: 0.0002    # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "    - name: ffn\n",
    "      patterns: []\n",
    "      lr: 0.0002  # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 15  # Maximum number of iterations\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 10    # Number of iterations for the first restart\n",
    "    T_mult: 10 # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_configs/config_test.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets / Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_train.pkl...\n",
      "Processing 3068 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accumulating:: 100%|██████████| 3068/3068 [00:00<00:00, 6311.81sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples due to errors.\n",
      "Loaded and processed 3068 samples.\n",
      "\n",
      "Loading dataset from: dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_val.pkl...\n",
      "Processing 383 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accumulating:: 100%|██████████| 383/383 [00:00<00:00, 6317.62sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples due to errors.\n",
      "Loaded and processed 383 samples.\n",
      "\n",
      "Loading dataset from: dataset/serialized/merged_ts=4-4_2bar_tr0.80-va0.10-te0.10_test.pkl...\n",
      "Processing 383 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accumulating:: 100%|██████████| 383/383 [00:00<00:00, 6316.77sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples due to errors.\n",
      "Loaded and processed 383 samples.\n",
      "\n",
      "Grid shape: torch.Size([16, 33, 9, 3])\n",
      "Samples shape: 16\n"
     ]
    }
   ],
   "source": [
    "## Load Datasets\n",
    "train_dataset = DrumMIDIDataset(\n",
    "    path     = config[\"data\"][\"train_path\"],\n",
    "    num_bars = config[\"data\"][\"num_bars\"],\n",
    "    feature_type = config[\"data\"][\"feature_type\"],\n",
    "    steps_per_quarter = config[\"data\"][\"steps_per_quarter\"],\n",
    "    subset   = config[\"data\"][\"subset\"]\n",
    ")\n",
    "\n",
    "val_dataset  = DrumMIDIDataset(\n",
    "    path     = config[\"data\"][\"val_path\"],\n",
    "    num_bars = config[\"data\"][\"num_bars\"],\n",
    "    feature_type      = config[\"data\"][\"feature_type\"],\n",
    "    steps_per_quarter = config[\"data\"][\"steps_per_quarter\"],\n",
    "    subset   = config[\"data\"][\"subset\"]\n",
    ")\n",
    "\n",
    "test_dataset = DrumMIDIDataset(\n",
    "    path     = config[\"data\"][\"test_path\"],\n",
    "    num_bars = config[\"data\"][\"num_bars\"],\n",
    "    feature_type      = config[\"data\"][\"feature_type\"],\n",
    "    steps_per_quarter = config[\"data\"][\"steps_per_quarter\"],\n",
    "    subset   = config[\"data\"][\"subset\"]\n",
    ")\n",
    "\n",
    "## Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = config[\"data\"][\"batch_size\"],\n",
    "    num_workers = config[\"data\"][\"num_workers\"],\n",
    "    shuffle = True,\n",
    "    collate_fn = train_dataset.collate_fn,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = config[\"data\"][\"batch_size\"],\n",
    "    num_workers = config[\"data\"][\"num_workers\"],\n",
    "    shuffle = True,\n",
    "    collate_fn = val_dataset.collate_fn,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = config[\"data\"][\"batch_size\"],\n",
    "    num_workers = config[\"data\"][\"num_workers\"],\n",
    "    shuffle = True,\n",
    "    collate_fn = test_dataset.collate_fn,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "## Test a sample\n",
    "for batch in train_loader:\n",
    "    print(f\"Grid shape: {batch['grid'].shape}\")\n",
    "    print(f\"Samples shape: {len(batch['samples'])}\")\n",
    "    grid = batch['grid']\n",
    "    random_idx = np.random.randint(len(batch['samples']))\n",
    "    sample = batch['samples'][random_idx]\n",
    "    sample.feature.play()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 33\n"
     ]
    }
   ],
   "source": [
    "NUM_QUARTERS_PER_BAR = 4 # 4/4 time signature\n",
    "MAX_LENGTH = config[\"data\"][\"num_bars\"] * NUM_QUARTERS_PER_BAR * config[\"data\"][\"steps_per_quarter\"] + 1\n",
    "print(f\"Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "GrooveIQ                                                     [16, 33, 9]               27\n",
       "├─DrumEncoderWrapper: 1-1                                    [16, 33, 9, 32]           --\n",
       "│    └─DrumAxialTransformer: 2-1                             [16, 33, 9, 32]           --\n",
       "│    │    └─Conv2d: 3-1                                      [16, 32, 33, 9]           128\n",
       "│    │    └─AxialPositionalEmbedding: 3-2                    [16, 32, 33, 9]           1,344\n",
       "│    │    └─Sequential: 3-3                                  [16, 32, 33, 9]           27,008\n",
       "├─Linear: 1-2                                                [16, 33, 9, 1]            33\n",
       "├─Linear: 1-3                                                [16, 33, 9]               297\n",
       "├─LearnableBinsQuantizer: 1-4                                [16, 33, 3]               8\n",
       "├─LearnableBinsQuantizer: 1-5                                [16, 33, 3]               16\n",
       "├─Linear: 1-6                                                [16, 33, 32]              896\n",
       "├─Linear: 1-7                                                [16, 33, 32]              320\n",
       "├─DrumDecoderWrapper: 1-8                                    [16, 33, 27]              --\n",
       "│    └─TransformerDecoder: 2-2                               [16, 33, 32]              --\n",
       "│    │    └─ModuleList: 3-4                                  --                        141,792\n",
       "│    │    └─LayerNorm: 3-5                                   [16, 33, 32]              64\n",
       "│    └─Linear: 2-3                                           [16, 33, 27]              891\n",
       "==============================================================================================================\n",
       "Total params: 172,824\n",
       "Trainable params: 172,824\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 93.49\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 29.28\n",
       "Params size (MB): 0.66\n",
       "Estimated Total Size (MB): 29.99\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = config[\"model\"]\n",
    "model_config.update(\n",
    "    T=MAX_LENGTH,\n",
    "    E=grid.shape[2],\n",
    "    M=grid.shape[3]\n",
    ")\n",
    "model = GrooveIQ(**model_config)\n",
    "summary(model, input_data = grid, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "🔧 Configuring Optimizer:\n",
      "├── Type: ADAMW\n",
      "├── Base LR: 0.0004\n",
      "├── Weight Decay: 1e-06\n",
      "├── Parameter Groups:\n",
      "│   ├── Group: self_attn\n",
      "│   │   ├── LR: 0.0002\n",
      "│   │   └── Patterns: []\n",
      "│   ├── Group: ffn\n",
      "│   │   ├── LR: 0.0002\n",
      "│   │   └── Patterns: []\n",
      "│   └── Default Group (unmatched parameters)\n",
      "└── AdamW Specific:\n",
      "    ├── Betas: [0.9, 0.999]\n",
      "    ├── Epsilon: 1e-08\n",
      "    └── AMSGrad: False\n",
      "\n",
      "📈 Configuring Learning Rate Scheduler:\n",
      "├── Type: COSINE\n",
      "├── Cosine Annealing Settings:\n",
      "│   ├── T_max: 15 epochs (2880 steps)\n",
      "│   └── Min LR: 1e-07\n",
      "├── Warmup Settings:\n",
      "│   ├── Duration: 5 epochs (960 steps)\n",
      "│   ├── Start Factor: 0.1\n",
      "│   └── End Factor: 1.0\n"
     ]
    }
   ],
   "source": [
    "trainer = GrooveIQ_Trainer(\n",
    "    model = model,\n",
    "    config = config,\n",
    "    run_name = config[\"expt\"],\n",
    "    config_file = config['training']['config_file'],\n",
    "    device = device\n",
    ")\n",
    "\n",
    "trainer.set_optimizer(\n",
    "    create_optimizer(\n",
    "        model=model,\n",
    "        opt_config=config['optimizer']\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer.set_scheduler(\n",
    "    create_scheduler(\n",
    "        optimizer=trainer.optimizer,\n",
    "        scheduler_config=config['scheduler'],\n",
    "        train_loader=train_loader,\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Metrics (Epoch 0):\n",
      "├── TRAIN:\n",
      "│   ├── hit_acc: 0.5605\n",
      "│   ├── hit_bce: 1.5207\n",
      "│   ├── hit_f1: 0.3359\n",
      "│   ├── hit_perplexity: 4.6709\n",
      "│   ├── hit_ppv: 0.2064\n",
      "│   ├── hit_tpr: 0.9168\n",
      "│   ├── joint_loss: 1.9110\n",
      "│   ├── offset_ahead: 0.2049\n",
      "│   ├── offset_behind: 0.3441\n",
      "│   ├── offset_mae: 0.0879\n",
      "│   ├── offset_mse: 0.0534\n",
      "│   ├── offset_penalty: 0.1111\n",
      "│   ├── offset_tightness: 0.4954\n",
      "│   ├── velocity_corr: 0.2903\n",
      "│   ├── velocity_mae: 0.3219\n",
      "│   ├── velocity_mse: 0.1519\n",
      "│   ├── velocity_penalty: 0.0739\n",
      "│   └── velocity_range_diff: 0.0508\n",
      "└── VAL:\n",
      "    ├── hit_acc: 0.4379\n",
      "    ├── hit_f1: 0.2433\n",
      "    ├── hit_ppv: 0.1459\n",
      "    ├── hit_tpr: 0.8085\n",
      "    ├── offset_ahead: 0.1928\n",
      "    ├── offset_behind: 0.4615\n",
      "    ├── offset_mae: 0.1144\n",
      "    ├── offset_mse: 0.0289\n",
      "    ├── offset_tightness: 0.3773\n",
      "    ├── velocity_corr: 0.1928\n",
      "    ├── velocity_mae: 0.3722\n",
      "    ├── velocity_mse: 0.2555\n",
      "    └── velocity_range_diff: 0.0503\n",
      "└── TRAINING:\n",
      "    └── learning_rate: 0.000112\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
